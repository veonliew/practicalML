---
title: "Practical Machine Learning Course Project"
author: "Wei Liew"
date: "4/14/2022"
output: 
  html_document: 
    keep_md: yes
keep_md: true
---

# Background and Overview 
Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively.  These type of devices are part of the quantified self movement – a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks.  One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it.  

In this project, we aim to use data from accelerometers on the belt, forearm, arm, and dumbbell of 6 participants, to predict the manner in which they did the exercise, which is represented in the “classe” variable in the training set.  They were asked to perform barbell lifts correctly and incorrectly in 5 different ways, which corresponds from Class A to Class E as below: 
•	Class A - exactly according to the specification 
•	Class B - throwing the elbows to the front
•	Class C - lifting the dumbbell only halfway
•	Class D - lowering the dumbbell only halfway
•	Class E - throwing the hips to the front  
Only Class A is the correct performance, and the other 4 classes correspond to other common mistakes. 
More information is available from the website here: http://groupware.les.inf.puc-rio.br/har (see the section on the Weight Lifting Exercise Dataset).

This report will address how the model was built, how cross validation was used, what the expected out-of-sample is, and why the choices were made.  Finally the prediction model will be used to predict 20 test cases. 

# Data Processing and Visualization
We started by loading the necessary libraries and dataset, followed by downloading the raw data and providing some basic data visualization.  
```{r setoptions&loaddata, echo=TRUE}
knitr::opts_chunk$set(echo = TRUE)
# Loading necessary libraries
library(caret)
library(rattle)
library(ggplot2)
```
```{r downloaddata&loaddata, echo = TRUE}
# Download the raw data file (training and testing data sets) from provided URLs
if(!file.exists("./data")) dir.create("./data")
trainUrl <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
testUrl <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
if (!file.exists("./data/pml-training.csv")) 
{ download.file(trainUrl, destfile="./data/pml-training.csv")}
if (!file.exists("./data/pml-testing.csv")) 
{ download.file(testUrl, destfile="./data/pml-testing.csv")}

# Load data into R by reading the generated csv file.
traincsv <- read.csv("./data/pml-training.csv")
testcsv <- read.csv("./data/pml-testing.csv")
```

```{r  datavisualization, echo = TRUE}
dim(traincsv)
dim(testcsv)
sum(is.na(traincsv))
sum(is.na(testcsv))
names(traincsv)[names(traincsv) != names(testcsv)]
names(testcsv)[names(traincsv) != names(testcsv)]
```

There are 19622 observations and 160 variables in the training data, while the testing data contains 20 observations and the same variables as the training set.  The outcome of this giving classification problem is determined to be the variable ‘classe’ in the last column in the training set.  However, the outcome variable ‘classe’ does not exist in the testing set, instead represented by another variable ‘problem_id’ as identification purposes of the 20 test cases for the submission of the prediction results.
  
There is a considerable number of NA or missing values.  Columns (predictors) that do not produce any information, such as NA values, will be removed to easier the analysis process.  

```{r  removena, echo = TRUE}
training <- traincsv[, colSums(is.na(traincsv)) == 0]
testing <- testcsv[, colSums(is.na(testcsv)) == 0]
dim(training)
dim(testing)
```
We will continue to remove the first seven predictors (such as X, user name, timestamps) since these variables do not make intuitive sense for prediction and believe to have little impact for the outcome classe.  We will also get rid of near zero variance predictors (zero covariates) as these variables are not useful when constructing a prediction model.

```{r  cleandata, echo = TRUE}
trainingA <- training[, -c(1:7)]
testingA <- testing[, -c(1:7)]
dim(trainingA)
dim(testingA)

#Remove near zero variance variables
nzv <- nearZeroVar(trainingA)
trainData <- trainingA[,-nzv]
dim(trainData)
nvzTest <- nearZeroVar(testingA)
nvzTest
testData <- testingA
dim(testData)
```

After the data cleaning process above, the number of variables (predictors) has been reduced to 53 from 160.  No near zero covariate for the testing data has been found.  It is shown that our training data have the same column numbers (predictors) as the testing data has for the prediction model.

# Prediction Model Building and Selection
To practice cross validation, the given training data is partitioned into 70% training and 30% testing (validation).  The original testing data (testData) will stay as is and will be used later for the final prediction of the 20 different test cases.

```{r  splittraindata , echo = TRUE}
set.seed(12345)
inTrain <- createDataPartition(trainData$classe, p = 0.7, list = FALSE)
train <- trainData[inTrain,]
valid <- trainData[-inTrain,]
dim(train)
dim(valid)
```
This project is a supervised machine learning problem since we clearly know our project objective and the categorical outcome variable, classe which takes values in a finite, unordered set.  Therefore, we will use supervised classification algorithms for the model prediction.  We will build and consider three different popular models, i.e. Decision Tree, Random Forests, and Gradient Boosted Tree on the training data (sub-training set).  Confusion Matrix will be performed to compare the different models by evaluating the accuracy of models on new data (sub-test set).  The prediction model with the highest accuracy will be selected to use on the untouched independent original testing set (testData) to fulfill the cross validation to predict the 20 different test cases, thus the result will be on an unbiased measurement of the out of sample accuracy of the model.

###TrainControl
We can specify the type of resampling or set options for how the model will be applied to the training data by creating the trainControl object below.  We use the cross validation method to avoid overfitting effects, to better estimate test error and for better model overall efficiency.  5-fold cross validation is considered when implementing the algorithms to save some computing time.

```{r  specifyresampling , echo = TRUE}
fitControl <- trainControl(method="cv", number=5)
```

##Decision Tree Model

```{r  decisiontreemodel, echo = TRUE}
#Train the model
rpartFit <- train(classe~., data= train, method="rpart", trControl=fitControl, tuneLength = 5)
rpartFit

#Prettier version of plot 
fancyRpartPlot(rpartFit$finalModel, cex = 0.7, tweak = 0.7)

#Predicting classe for validation set (valid) using classification tree model  
pred_rpart <- predict(rpartFit, newdata = valid)

#Compare and print results of models 
cm_rpart <- confusionMatrix(factor(valid$classe), pred_rpart)
cm_rpart
```
The accuracy has been found to be 0.5414 (55.14%), which is considerable low and the expected out of sample error rate is estimated to be around 44.86%, which is very high. 

##Random Forests Model
```{r  randomforestsmodel, echo = TRUE}
#Train the model
rfModFit <- train(classe~., data= train, method = "rf", trControl=fitControl, tuneLength = 5)
rfModFit

#Predicting classe for validation set (valid) using random forests model  
predRF <- predict(rfModFit, newdata = valid)

#Compare and print results of models 
cmRF <- confusionMatrix(factor(valid$classe), predRF)
cmRF
```

The accuracy has been found to be 0.9948 (99.48%), which is considerable high and the expected out of sample error rate is estimated to be around 0.52%, which is very low. 

Below are the two reference plots for the random forests model.
```{r  accuracy_noofpreditorsplot, echo = TRUE}
p1 <- plot(rfModFit, main="Accuracy of Random Forest Model by number of predictors")
p1
```

```{r  error_nooftreesplot, echo = TRUE}
p2 <- plot(rfModFit$finalModel, main = "Model Error of Random Forest model by number of trees")
p2
```

## Generalize Boosted Models
```{r  generalizedboostedmodel, echo = TRUE}
gbmModFit <- train(classe~., data=train, method="gbm", trControl= fitControl, tuneLength = 5, verbose=FALSE)

#Predicting classe for validation set (valid) using generalized boosted model 
predGBM <- predict(gbmModFit, newdata = valid)

#Compare and print results of models 
cmGBM <- confusionMatrix(factor(valid$classe), predGBM)
cmGBM
```

### Summarized Accuracy and Out of Sample Error
```{r  summarizedaccuracy&ooserror, echo = TRUE}
AccuracyResults <- data.frame("Model" = c("Tree(rpart)", "RF", "GBM"), "Accuracy" = c(cm_rpart$overall[1], cmRF$overall[1], cmGBM$overall[1]), "Out of Sample Error" = c(1 -  cm_rpart$overall[1], 1 - cmRF$overall[1], 1 -  cmGBM$overall[1]))
AccuracyResults
```

#Predictions on (Validation) Test Data

The next possible step might be using the ensembling methods in learning, to combine classifiers by averaging or voting to improve accuracy in the model.  However, given the high accuracy of two models (Random Forests and Generalized Boosting Models) above, and the interpretability reductions and computational complexity increase of the ensembling methods, we will skip these further research efforts for now.
We will use the Random Forests Model that has the highest accuracy as our prediction model to predict the classe for the given 20 different test cases on the original test data. 

```{r  predict20testcases, echo = TRUE}
pred <- predict(rfModFit, newdata = testData)
pred
```

#Conclusions
Based on our analysis above on the prediction model building and selection of the 3 models, i.e. decision trees, random forests, and generalized gradient boosting models, we concluded that the random forests model (with cross validation) with the highest accucracy of 0.9959 and the expected out of sample error of 0.004078 is the best predictive model to use to predict the test cases.    

The outcome of the 20 test cases prediction is listed as below:
```{r  testdatapredictions, echo = TRUE}
testDataPredResults <- data.frame ("problem_id" = testData$problem_id, "predicted_classe" = pred)
testDataPredResults
```
